{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задание по много поточности:\n",
    "\n",
    "Вам необходимо проанализировать википедию на предмет того, какие слова в каждой из частей речи встречаются чаще. Вы хотите реализовать это в несколько потоков.\n",
    "\n",
    "Запросы к википедии можно осуществлять с помощью библиотеки wikipedia. Для морфологического анализа используйте библиотеку pymorphy2. Чтобы разбить текст на слова можете воспользоваться функцией word_tokenize из библиотеки nltk.\n",
    "\n",
    "Класс должен иметь функции, приведенные ниже (но может иметь и другие на ваше усмотрение)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1\n",
    "\n",
    "<b>Многопоточной реализация</b>\n",
    "\n",
    "Задачи делятся на три типа:\n",
    "<ul>\n",
    "<li><i>Получение данных</i>:\n",
    "<ol>\n",
    "<li>Получение заголовков для страниц википедии - запускает по max_threads функций, которые асинхронно получают заголовки страниц.</li>\n",
    "<li>Получение конкретных страниц - ждем, пока не появятся новые заголовки, которые не обработаны.\n",
    "Когда появились - начинаем запрашивать в max_threads функциях конкретные страницы по заголовкам.</li>\n",
    "</ol>\n",
    "</li>\n",
    "<li><i>Обработка данных</i>:\n",
    "<ol>\n",
    "<li>Ждем, пока не появятся новые необработанные страницы. Когда появляются, запускаем по max_threads функций для морфологического анализа слов.</li>\n",
    "<li>Ждем пока не появились обработанные слова. Как только появляется новое слово, сразу же обновляем _stats.</li>\n",
    "</ol>\n",
    "</li>\n",
    "<li><i>Сохранение данных</i>:\n",
    "<ol>\n",
    "        Раз в store_every обработанных слов вызывается асинхронно функция dump, которая сохраняет _stats.\n",
    "</ol>\n",
    "</li>\n",
    "</ul>\n",
    "<b>P. S.</b>\n",
    "\n",
    "Комментарии специально запутанные, чтобы вы сами придумали архитектуру вызова потоков. Не бойтесь использовать Queue и daemon=True. Запрещается использовать threading.Lock / threading.RLock или другие блокировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from collections import defaultdict\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArticle():\n",
    "    page_title = wikipedia.random(pages=1)\n",
    "    return page_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetWordsList(article):\n",
    "    while True:\n",
    "        try:\n",
    "            page = wikipedia.page(title=article)\n",
    "            tokens = nltk.word_tokenize(page.content)\n",
    "            tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "            break\n",
    "        except:\n",
    "            print(\"warning: couldn't get {}, retry\".format(article))\n",
    "            article = GetArticle()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        title  = GetArticle()\n",
    "        words = GetWordsList(title)[:10]\n",
    "        print(words)\n",
    "    except:\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_job_searcher(tasks_queue, articles_queue):\n",
    "    while True:\n",
    "        item = tasks_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        article = GetArticle()\n",
    "        articles_queue.put(article)\n",
    "        tasks_queue.task_done()\n",
    "    articles_queue.put(None)\n",
    "        \n",
    "def thread_job_proccesor_titles(articles_queue, words_queue):\n",
    "    while True:\n",
    "        item = articles_queue.get()\n",
    "#         print(\"processing {}\".format(item))\n",
    "        if item is None:\n",
    "            break\n",
    "        words = GetWordsList(item)\n",
    "        words_queue.put(words)\n",
    "        articles_queue.task_done()\n",
    "    words_queue.put(None)\n",
    "    \n",
    "def thread_job_proccesor_pages(words_queue, morph_queue):\n",
    "    while True:\n",
    "        item = words_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        print(\"processing {}\".format(item[:10]))\n",
    "        morph = pymorphy2.MorphAnalyzer()\n",
    "        for word in item:\n",
    "            ans = morph.parse(word)[0].tag.POS\n",
    "            morph_queue.put(ans)\n",
    "        words_queue.task_done()\n",
    "    morph_queue.put(None)\n",
    "        \n",
    "def thread_job_proccesor_words(morph_queue, stats):\n",
    "    while True:\n",
    "        item = morph_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        print(\"processing {}\".format(item))\n",
    "        stats[item] += 1\n",
    "        morph_queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 5\n",
    "count_articles = 10\n",
    "tasks_queue = Queue()\n",
    "articles_queue = Queue()\n",
    "words_queue = Queue()\n",
    "morph_queue = Queue()\n",
    "stats = defaultdict(int)\n",
    "\n",
    "for i in range(count_articles):\n",
    "    tasks_queue.put(i)\n",
    "    \n",
    "for i in range(num_threads):\n",
    "    tasks_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ['jill', 'ann', 'sterkel', 'born', 'may', 'is', 'an', 'american', 'former', 'competition']\n",
      "processing ['astrapogon', 'is', 'a', 'genus', 'of', 'cardinalfishes', 'native', 'to', 'the', 'western']\n",
      "processing ['aapka', 'tv', 'is', 'an', 'telugu', 'and', 'hindi', 'news', 'and', 'social']\n",
      "processing ['onibury', 'railway', 'station', 'was', 'a', 'station', 'in', 'onibury', 'shropshire', 'england']\n",
      "processing ['arthur', 'whitelock', 'lemon', 'april', 'may', 'was', 'a', 'welsh', 'international', 'number']\n",
      "processing ['angelo', 'paravisi', 'september', 'september', 'was', 'the', 'bishop', 'of', 'crema', 'from']\n",
      "processing ['the', 'tornado', 'is', 'a', 'wooden', 'roller', 'coaster', 'located', 'at', 'adventureland']\n",
      "processing ['the', 'kosovo', 'national', 'under', 'football', 'team', 'albanian', 'përfaqësuesja', 'e', 'futbollit']\n",
      "processing ['eady', 'is', 'a', 'surname', 'notable', 'people', 'with', 'the', 'surname', 'include']\n",
      "processing ['shaun', 'mark', 'bean', 'born', 'april', 'known', 'professionally', 'as', 'sean', 'bean']\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "for i in range(num_threads):\n",
    "    t = threading.Thread(target=thread_job_searcher, args=(tasks_queue, articles_queue))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    t = threading.Thread(target=thread_job_proccesor_titles, args=(articles_queue, words_queue))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "    t = threading.Thread(target=thread_job_proccesor_pages, args=(words_queue, morph_queue))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "    t = threading.Thread(target=thread_job_proccesor_words, args=(morph_queue, stats))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"cat\")[0].tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiReader(object):\n",
    "    \"\"\"\n",
    "    Класс для работы с википедией.\n",
    "    Собирает статисткику по словам каждой части речи в статьях википедии.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    morphs: list\n",
    "        Части речи, которые хотим исследовать. Слова другой части речи не включаются в статистику.\n",
    "    \n",
    "    page_per_req: int\n",
    "        Количество случайных названий страниц, запрашиваемых за один раз у википедии.\n",
    "    \n",
    "    max_threads: int\n",
    "        Количество потоков, запускаемых другим потоком демоном (можно не использовать, если получится).\n",
    "    \n",
    "    max_words: int\n",
    "        Количество слов для обработки.\n",
    "    \n",
    "    store_every: int\n",
    "        Как часто сохранять данные на диск. Каждые store_every слов вызывается функция dump.\n",
    "    \n",
    "    store_path: str\n",
    "        Куда сохранять данные.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    _stats: <your code here>\n",
    "        Структура данных (возможно встроенная), позволяющая хранить для каждой части речи список слов с их количеством.\n",
    "        Необходимо, чтобы получение (изменение) статистики (текущего количества) для каждой пары\n",
    "        <часть речи, слово> происходило за O(1).\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 morphs=[],\n",
    "                 page_per_req=4,\n",
    "                 max_threads_per_daemon=8,\n",
    "                 max_words=10000,\n",
    "                 store_every=1000,\n",
    "                 store_path=\"\"):\n",
    "        self._stats = {}\n",
    "    \n",
    "    def run(self):\n",
    "        <your code here>\n",
    "    \n",
    "    def dump(self, path=None):\n",
    "        <your code here>\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        <your code here>\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
